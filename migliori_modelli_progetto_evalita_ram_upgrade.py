# -*- coding: utf-8 -*-
"""MIGLIORI MODELLI Progetto_Evalita_RAM_UPGRADE.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kD8qI0EOhr1LWNVLxBxbavJifTeHNPnC
"""

!pip install tweet-preprocessor
!pip install tqdm
!pip install emoji
!pip install fasttext

import tensorflow as tf
print(tf.__version__)

import preprocessor as p
import re
import numpy as np
import nltk
nltk.download('stopwords')
nltk.download('punkt')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import pandas as pd
from nltk.stem import SnowballStemmer
from tqdm import tqdm
tqdm.pandas()
import csv
import emoji
len(stopwords.words('italian'))
from google.colab import drive
drive.mount('/content/drive')

import sklearn
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix

import matplotlib.pyplot as plt
import gzip

from keras import layers
from keras.models import Sequential
from keras.layers import Dense, Activation
from keras.preprocessing.text import Tokenizer, one_hot
from keras.preprocessing.sequence import pad_sequences

from string import punctuation
import string

#new column in the dataframe
df_cols = ["Testo", "Odio", "Stereotipo"]

df = pd.read_table('/content/drive/MyDrive/Progetto Evalita/haspeede2_dev_taskAB.tsv', delimiter="\t|\t0",encoding='utf-8', names = df_cols, engine='python')
df = df.drop(df.index[0])

#change values in the column label based on values
df.Stereotipo.replace(to_replace ="1", value = 1, inplace = True)
df.Stereotipo.replace(to_replace ="0", value = 0, inplace = True)

df = df[["Testo","Stereotipo"]]
df

print(df['Testo'].iloc[6808])

def custom_cleaning(text):
    # rimuovere simboli e converto le emoji in parole
    text  = " " + emoji.demojize(text, delimiters=(" ", " "), language="it")
    text  = "".join([char for char in text if char not in '"%\()*+-/<=>[]^_`{|}~�✔'])   
    text = re.sub(r"[,.;@#?!&$]+\ *", " ", text)
    
    # rimuovo le stopwords
    text = text.lower().split()
    text = [w for w in text if not w in stopwords.words('italian') and len(w) >= 3]
    text = " ".join(text)

    # rimuovere numeri e stringhe più caratteristiche dei dati
    text = re.sub('[0-9]+', '', text)
    text = re.sub('user', '', text)
    text = re.sub('url', '', text)
    text = re.sub("'", ' ', text)
    text = re.sub("…", ' ', text)
    text = re.sub("#", ' ', text)
    
    
    return text

def stemming(text):

  text = text.split()
  stemmed_words = [SnowballStemmer('italian').stem(word) for word in text] 
  text = " ".join(stemmed_words)

  return text

def clean_text(text):
  
  # I° pulizia del testo (stopwords, simboli, coversione emoji, caratteri non necessari)
  text_cleaned = custom_cleaning(text)

  # II° pulizia del testo tramite la libreria per i tweet
  p.set_options(p.OPT.NUMBER, p.OPT.URL)
  text_cleaned = p.clean(text_cleaned)

  # Stemmming del testo pulito
  #Ricordarsi di commentare quando usiamo il vocabolario di FASTTEXT poichè risulta migliore senza lo steamming delle parole
  text_cleaned = stemming(text_cleaned) 

  return text_cleaned

df['Testo'] = df['Testo'].progress_map(lambda x: clean_text(x))

X = df['Testo'].fillna('').to_list()
Y = df['Stereotipo'].fillna('').to_list()

print(X[15])
print(Y[15])

#Contiano la lunghezza max e min delle frasi con e senza stereotipi
#e conto il numero di classi per gli stereotipi e non.
maxS = 0
minS = 0
maxNS = 0
minNS = 0
n_Stereotipi = 0 
n_NonStereotipi = 0 
FraseMaxS = ''
FraseMaxNS = ''

for ex in tqdm(range(len(X))):
  if Y[ex] == 1:
    n_Stereotipi += 1
    if len(X[ex]) > maxS:
      maxS = len(X[ex])
      FraseMaxS = X[ex]
    if len(X[ex]) < minS or minS == 0:
      minS = len(X[ex])
  else:
    n_NonStereotipi += 1
    if len(X[ex]) > maxNS:
      maxNS = len(X[ex])
      FraseMaxNS = X[ex]
    if len(X[ex]) < minNS or minNS == 0:
      minNS = len(X[ex])

print("Num Stereotipi: " + str(n_Stereotipi))
print("Num non Stereotipi: " + str(n_NonStereotipi))
print("Lunghezza Min Stereotipi: " + str(minS))
print("Lunghezza Max Stereotipi: " + str(maxS))
print(FraseMaxS)
print("Lunghezza Min Non Stereotipi: " + str(minNS))
print("Lunghezza Max Non Stereotipi: " + str(maxNS))
print(FraseMaxNS)

RANDOM_STATE = 42

# Split train & test
text_train, text_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=RANDOM_STATE)
print(text_train[0])
print(text_train[1])

#TO DO : 
#Tokenize and transform to integer index
tokenizer = Tokenizer()

tokenizer.fit_on_texts(text_train)

X_train = tokenizer.texts_to_sequences(text_train)
X_test  = tokenizer.texts_to_sequences(text_test)

#TO DO: get some statistics
vocab_size = len(tokenizer.word_index) + 1 # adding 1 because of reserverd 0 index

print("vocabulary size: ", vocab_size)

maxlen_train = max( len(x) for x in X_train)
maxlen_test = max( len(x) for x in X_test)

minlen_train = min( len(x) for x in X_train)
minlen_test = min( len(x) for x in X_test)

print("maxlen train dataset: ", maxlen_train)
print("maxlen test dataset: ", maxlen_test)

print("minlen train dataset: ", minlen_train)
print("minlen test dataset: ", minlen_test)


tokenizer.get_config()

print("Esempio di un vettore di train :")
print(str(X_train[0]))
print("--------------------------")
print("Shape del train " + str(X_train.shape))

# Add pading to ensure all vectors have same dimensionality
maxlen = maxlen_train

X_train = pad_sequences(X_train, padding = "post", maxlen = maxlen)
X_test = pad_sequences(X_test, padding = "post", maxlen = maxlen)

print("Esempio di un vettore di train :")
print(str(X_train[0]))
print("--------------------------")
print("Shape del train " + str(X_train.shape))

print("Esempio di un vettore di test :")
print(str(X_test[0]))
print("--------------------------")
print("Shape del test " + str(X_test.shape))

def create_model(embedding_dim):

  model_dense = tf.keras.Sequential()
  model_dense.add(tf.keras.layers.Embedding(vocab_size,embedding_dim, input_length = maxlen))
  model_dense.add(tf.keras.layers.Flatten())
  model_dense.add(tf.keras.layers.Dense(64, activation = "relu", bias_initializer=tf.keras.initializers.Constant(0.1)))
  model_dense.add(tf.keras.layers.Dropout(0.3))
  model_dense.add(tf.keras.layers.Dense(32, activation = "relu", bias_initializer=tf.keras.initializers.Constant(0.1)))
  model_dense.add(tf.keras.layers.Dropout(0.3))
  model_dense.add(tf.keras.layers.Dense(16, activation = "relu", bias_initializer=tf.keras.initializers.Constant(0.1)))
  model_dense.add(tf.keras.layers.Dropout(0.3))
  model_dense.add(tf.keras.layers.Dense(1, activation = "sigmoid", bias_initializer="zeros"))

  model_dense.compile(optimizer = "adam", loss ="binary_crossentropy", metrics = ["accuracy"] )
  return model_dense

model_dense = create_model(100) #Embedding Dim
model_dense.summary()

# Fit model and get the history
history = model_dense.fit(np.array(X_train), np.array(y_train), epochs=10,verbose=True, validation_data=(np.array(X_test), np.array(y_test)),batch_size=1024)

tf.keras.utils.plot_model(model_dense, show_shapes=True, show_dtype=True,
    show_layer_names=True, rankdir='TB', expand_nested=True, dpi=300
)

#TO DO : evaluate the model 
loss, accuracy = model_dense.evaluate(np.array(X_test), np.array(y_test), verbose = 1)

plt.style.use('ggplot')

def plot_history(history):
    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']
    loss = history.history['loss']
    val_loss = history.history['val_loss']
    x = range(1, len(acc) + 1)

    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(x, acc, 'b', label='Training acc')
    plt.plot(x, val_acc, 'r', label='Validation acc')
    plt.title('Training and validation accuracy')
    plt.legend()
    plt.subplot(1, 2, 2)
    plt.plot(x, loss, 'b', label='Training loss')
    plt.plot(x, val_loss, 'r', label='Validation loss')
    plt.title('Training and validation loss')
    plt.legend()
    plt.show()

plot_history(history)

y_pred = model_dense.predict_classes(X_test)
errori_commessi = 0
corretto = 0
count = 0
print()
for i in range(0, y_pred.size):
    count += 1
    if(np.around(y_pred[i], decimals = 0) != y_test[i]):
        errori_commessi+=1
    else:
        corretto+=1

from sklearn.metrics import accuracy_score
print("Accuratezza: ", accuracy_score(y_test,y_pred))
print()
print('Dimensione training set: ', np.array(y_train).size)
print('Dimensione test set: ', np.array(y_pred).size)
print()
print('Previsioni Corrette: ', corretto)
print('Errori Commessi: ', errori_commessi)
accuratezza = round((corretto/np.array(y_pred).size*100),2)
print('Accuratezza: ', accuratezza,'%')
print()
print(classification_report(y_test,y_pred))

import matplotlib.pyplot as plt
from mlxtend.plotting import plot_confusion_matrix
cm = confusion_matrix(y_test,y_pred)
fig, ax = plot_confusion_matrix(conf_mat=cm)
plt.show()

#predizione di una frase 
x_sample = input("inserisci il tweet da predire: ")
print(x_sample)
x_sample = clean_text(x_sample)
x_sample=[x_sample]
print(x_sample)
x_sample_token = tokenizer.texts_to_sequences(x_sample)
print(x_sample_token)

x_sample_token = pad_sequences(x_sample_token, padding = "post", maxlen = maxlen)
print(x_sample_token)

print("La predizione è: ")
predizione = model_dense.predict_classes(x_sample_token)
print(predizione)

"""MODELLO CON VOCABOLARIO FASTTEXT

"""

print(X[15])
print(Y[15])

import fasttext
import fasttext.util

ft = fasttext.load_model('/content/drive/MyDrive/Progetto Evalita/cc.it.100.bin')

ft.get_word_vector('gatto').shape

ft.get_nearest_neighbors('gatto')

"""creo matrice e modello con fasttext"""

embedding_dim = 100

embeddings_matrix = np.zeros((vocab_size, embedding_dim)) 
print(vocab_size)

count1 = 0
count2 = 0
for word, index in tokenizer.word_index.items():
  if index > vocab_size-1:
    break
  embedding_vector = ft.get_word_vector(word)
  if embedding_vector is not None:
    embeddings_matrix[index] = embedding_vector
    count1 += 1
  else:
    count2 += 1
print("Parole del nostro vocabolario presenti nella matrice: "+ str(count1))
print("Parole del nostro vocabolario non presenti nella matrice: "+ str(count2))

model_prova = tf.keras.Sequential()
model_prova.add(tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=maxlen, weights = [embeddings_matrix], trainable = False))
model_prova.add(tf.keras.layers.Flatten())
model_prova.add(tf.keras.layers.Dense(128, activation='relu', bias_initializer='zeros' ))
model_prova.add(tf.keras.layers.Dense(64, activation='relu', bias_initializer='zeros' ))
model_prova.add(tf.keras.layers.Dense(32, activation='relu', bias_initializer='zeros' ))
model_prova.add(tf.keras.layers.Dense(1, activation='sigmoid', bias_initializer='zeros' ))
model_prova.compile(optimizer = "adam", loss ="binary_crossentropy", metrics = ["accuracy"] )
model_prova.summary()

history = model_prova.fit(np.array(X_train), np.array(y_train), epochs=4,verbose=True, validation_data=(np.array(X_test), np.array(y_test)),batch_size=128)

loss, accuracy = model_prova.evaluate(np.array(X_test), np.array(y_test), verbose = 1)

plt.style.use('ggplot')

def plot_history(history):
    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']
    
    loss = history.history['loss']
    val_loss = history.history['val_loss']
    x = range(1, len(acc) + 1)

    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(x, acc, 'b', label='Training acc')
    plt.plot(x, val_acc, 'r', label='Validation acc')
    plt.title('Training and validation accuracy')
    plt.legend()
    plt.subplot(1, 2, 2)
    plt.plot(x, loss, 'b', label='Training loss')
    plt.plot(x, val_loss, 'r', label='Validation loss')
    plt.title('Training and validation loss')
    plt.legend()
    plt.show()

plot_history(history)

y_pred = model_prova.predict_classes(X_test)#, batch_size = 20, verbose = 1)
errori_commessi = 0
corretto = 0
count = 0
print()
for i in range(0, y_pred.size):
    count += 1
    if(np.around(y_pred[i], decimals = 0) != y_test[i]):
        errori_commessi+=1
    else:
        corretto+=1

cm = confusion_matrix(y_test,y_pred)

print()
print()
print('Utilizzati come training set: ', np.array(y_train).size)
print('Utilizzati come test set: ', np.array(y_pred).size)
print()
print('Previsioni Corrette: ', corretto)
print('Errori Commessi: ', errori_commessi)
accuratezza = round((corretto/np.array(y_pred).size*100),2)
print('Accuratezza: ', accuratezza,'%')
print()
print(classification_report(y_test,y_pred))
print('Matrice di Confusione:')
print(cm)

#or just use the sklearn accuracy metric
from sklearn.metrics import accuracy_score
print("Accuratezza con la funzione: ", accuracy_score(y_test,y_pred))

import matplotlib.pyplot as plt
from mlxtend.plotting import plot_confusion_matrix

fig, ax = plot_confusion_matrix(conf_mat=cm)
plt.show()

#predizione di una frase 
x_sample = input("inserisci il tweet da predire: ")
print(x_sample)
x_sample = clean_text(x_sample)
x_sample=[x_sample]
print(x_sample)
x_sample_token = tokenizer.texts_to_sequences(x_sample)
print(x_sample_token)

x_sample_token = pad_sequences(x_sample_token, padding = "post", maxlen = maxlen)
print(x_sample_token)

print("La predizione è: ")
predizione = model_prova.predict_classes(x_sample_token)
print(predizione)

"""**CAPIRE DOVE SBAGLIA IL MODELLO**"""

for i in range(0, len(Test_label)):

  x_sample=[Test_record[i]]
  x_sample_token = tokenizer.texts_to_sequences(x_sample)
  x_sample_token = pad_sequences(x_sample_token, padding = "post", maxlen = maxlen)
  predizione = model_dense.predict_classes(x_sample_token)
  if predizione[0][0] != Test_label[i]:
      print("Il modello ha predetto "+str(predizione[0][0]))
      print("Il record era classe "+str(Test_label[i]))
      print(Test_record[i])